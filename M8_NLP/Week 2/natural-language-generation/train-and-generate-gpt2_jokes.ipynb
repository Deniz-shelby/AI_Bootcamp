{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import json\n",
    "\n",
    "# collect sentences\n",
    "with open(\"dataset_jokes/wocka.json\") as fn:\n",
    "  jokes = json.load(fn)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "jokes[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'body': 'What do you call a cow with no legs?\\r\\n\\r\\nGround Beef!',\n",
       " 'category': 'Animal',\n",
       " 'id': 1,\n",
       " 'title': 'Cow With No Legs'}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "sentences = [] # collect sentences\n",
    "\n",
    "for i in jokes: # iterate over all recipes\n",
    "    try:\n",
    "        title = i['title'] # get the title\n",
    "        category = i['category'] # get the category\n",
    "        body = i['body'] # get the body\n",
    "        sentence = f\"{title}, {category}, {body}\" # create the sentence as string\n",
    "        if sentence != '': # if the sentence is not empty\n",
    "            sentences.append(sentence) # add the sentence to the list\n",
    "    except KeyError: # if the recipe has no title or ingredients\n",
    "        continue\n",
    "\n",
    "# clean sentences\n",
    "# TODO: add further cleaning steps\n",
    "def clean(sentence):\n",
    "    sentence = sentence.replace('\\r', ' ')  # replace repetetive words\n",
    "    sentence = sentence.replace('\\n', '')  # replace new line chars\n",
    "    sentence = sentence.replace('  ', ' ')  # replace repetetive words\n",
    "    sentence = sentence.strip()  # strip leading and trailing white-spaces\n",
    "    return sentence\n",
    "\n",
    "sentences = list(map(clean, sentences))  # map method.\n",
    "# sentences = [clean(sentence) for sentence in sentences]  # list comprehension method"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "sentences[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'Cow With No Legs, Animal, What do you call a cow with no legs? Ground Beef!'"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split into train/dev\n",
    "# TODO: alternatively, we could use the `datasets.Dataset.train_test_split()` method \n",
    "SEED = 10  # set seed var for reproducibility\n",
    "train_sentences, test_sentences = train_test_split(sentences, \n",
    "                                                   test_size=0.1, \n",
    "                                                   # change the train_size for rapid testing (for example, use 0.1)\n",
    "                                                   train_size=0.9,  \n",
    "                                                   random_state=SEED)\n",
    "\n",
    "# write into files\n",
    "for split, sents in zip(['train', 'test'], [train_sentences, test_sentences]):\n",
    "    with open(f\"{split}.txt\", 'w') as fn:\n",
    "        fn.write('\\n'.join(sents))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "# create the datasets.Dataset object\n",
    "from datasets import load_dataset \n",
    "\n",
    "dataset = load_dataset('text', data_files={'train': 'train.txt', 'test': 'test.txt'}) # load the dataset from the text files"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using custom data configuration default-6a30f5af75f6bfd4\n",
      "                            "
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /home/deniz/.cache/huggingface/datasets/text/default-6a30f5af75f6bfd4/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
      "Dataset text downloaded and prepared to /home/deniz/.cache/huggingface/datasets/text/default-6a30f5af75f6bfd4/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "dataset"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 9052\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "# Instantiate tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "pretrained_model = 'gpt2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=pretrained_model)\n",
    "\n",
    "# Define a function to tokenize the dataset and return the text indices. \n",
    "# We also add trailing <|endoftext|> special token\n",
    "def tokenize_sentence(dataset):\n",
    "    # As we can see, there is no padding since the PAD token is not originally used by GPT-2. \n",
    "    # We could perform padding by adding the PAD token to the vocabulary with the method `add_special_tokens()`\n",
    "    return tokenizer([f\"{sentence} {tokenizer.eos_token}\" for sentence in dataset['text']])\n",
    "    # return tokenizer(dataset['text])\n",
    "\n",
    "# apply to dataset object\n",
    "dataset_features = dataset.map(tokenize_sentence,\n",
    "                               batched=True,\n",
    "                               remove_columns=['text'],\n",
    "                               desc='Tokenizing train and test splits')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Tokenizing train and test splits:   0%|          | 0/10 [00:00<?, ?ba/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1127 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing train and test splits: 100%|██████████| 10/10 [00:00<00:00, 14.49ba/s]\n",
      "Tokenizing train and test splits: 100%|██████████| 2/2 [00:00<00:00, 23.41ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "dataset_features"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['attention_mask', 'input_ids'],\n",
       "        num_rows: 9052\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['attention_mask', 'input_ids'],\n",
       "        num_rows: 1002\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# group sentences in batches of equal size (standard GPT-2 approach)\n",
    "# We use an adaptation of the `group_text` function for that purpose\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    block_size = 512  # set the \"blocks\" to half of the maximum GPT-2 model length (1024) for memory issues\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i: i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "\n",
    "    # # Add labels to the dataset_features\n",
    "    # # Since the task is language modelling, the labels to predict are actually the input indices \"shifted\"\n",
    "\n",
    "    # result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# apply the group function to the dataset\n",
    "\n",
    "dataset_grouped = dataset_features.map(group_texts,\n",
    "                                       batched=True,\n",
    "                                       desc='Group sentences in blocks of equal size (512)')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Group sentences in blocks of equal size (512): 100%|██████████| 10/10 [00:03<00:00,  2.60ba/s]\n",
      "Group sentences in blocks of equal size (512): 100%|██████████| 2/2 [00:00<00:00,  5.22ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Check block size "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for i in dataset_grouped['train']['input_ids']:\n",
    "    if len(i) != 512:\n",
    "        print(len(i))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "for i in dataset_grouped['test']['input_ids']:\n",
    "    if len(i) != 512:\n",
    "        print(i)\n",
    "        print(len(i))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[4834, 15521, 972, 11, 8366, 11, 4874, 612, 373, 257, 2576, 508, 2227, 4025, 17515, 11, 523, 530, 1110, 673, 1816, 284, 766, 607, 6253, 11, 1583, 13, 4176, 13, 220, 1583, 13, 4176, 1297, 607, 284, 6437, 607, 17515, 290, 9585, 262, 1708, 25, 366, 6173, 6684, 3483, 36, 11, 35, 6684, 3483, 36, 11, 43, 6684, 3483, 36, 11, 314, 41300, 26746, 30373, 347, 6684, 3483, 1546, 1911, 1881, 1110, 673, 373, 2491, 2739, 11, 290, 3066, 284, 466, 607, 13565, 319, 262, 1323, 618, 257, 3516, 1625, 510, 284, 607, 290, 1965, 611, 673, 373, 257, 5827, 286, 1583, 13, 4176, 338, 11, 284, 543, 673, 8712, 25, 366, 5297, 11, 703, 750, 345, 760, 43634, 679, 8712, 366, 39, 11860, 15513, 360, 11860, 15513, 37760, 2474, 220, 50256, 38101, 29926, 2611, 1406, 12301, 11, 25455, 337, 2002, 64, 11, 25455, 40084, 523, 3735, 326, 673, 17157, 625, 290, 1392, 5169, 329, 6301, 8469, 13, 220, 50256]\n",
      "160\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "# Add \"labels\" column to the dataset_features. \n",
    "# To modify the dataset structure, we use the `dataset.map()` method\n",
    "def add_labels(dataset):\n",
    "    # Since the task is language modelling, the labels to predict are actually \n",
    "    # the input indices shifted forward by one element (token)\n",
    "    dataset['labels'] = dataset['input_ids'].copy()\n",
    "    return dataset\n",
    "\n",
    "dataset_for_lm = dataset_grouped.map(add_labels,\n",
    "                                     batched=True,\n",
    "                                     desc='Add labels to create data for language model training')\n",
    " "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Add labels to create data for language model training: 100%|██████████| 3/3 [00:00<00:00,  4.11ba/s]\n",
      "Add labels to create data for language model training: 100%|██████████| 1/1 [00:00<00:00, 14.45ba/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "# Instantiate the model class\n",
    "from transformers import (\n",
    "    AutoConfig, \n",
    "    AutoModelForCausalLM, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "import torch\n",
    "\n",
    "\n",
    "# TODO: experiment with different model configuration and batch sizes until \n",
    "# the models fits into GPU memory (otherwise it generated CUDA-out-of-memory error)\n",
    "# The model is instantiated from the pretrained GPT-2 model\n",
    "# Here, I reduced the number of attention head and layers, \n",
    "# to significantly reduce the model size and make sure it fits in the GPU memory\n",
    "config = AutoConfig.from_pretrained(pretrained_model,\n",
    "                                    n_head=12,  # reduce the size of the model for memory issues\n",
    "                                    n_layer=12)\n",
    "\n",
    "pretrained_model = 'gpt2-recipes'\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model, \n",
    "                                             config=config)\n",
    "\n",
    "# Again, we simulate a batch size of 8 by setting the `gradient_accumulation_steps` parameters\n",
    "no_cuda = not bool(torch.cuda.is_available())\n",
    "\n",
    "if no_cuda:\n",
    "  print(f\"Training on CPUs\")\n",
    "else:\n",
    "  print(f\"Training on GPU\")\n",
    "\n",
    "training_args = TrainingArguments(no_cuda=no_cuda,\n",
    "                                  per_device_train_batch_size=4,\n",
    "                                  per_device_eval_batch_size=4,\n",
    "                                  gradient_accumulation_steps=4, # virtually increment the batch_size\n",
    "                                  evaluation_strategy='epoch',\n",
    "                                  save_strategy='epoch',\n",
    "                                  logging_steps=100,\n",
    "                                  logging_dir='gpt2-jokes/tb',  # where to store the tensorboard\n",
    "                                  num_train_epochs=2,\n",
    "                                  output_dir='gpt2-jokes')\n",
    "\n",
    "# Start the training!\n",
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_for_lm['train'],\n",
    "    eval_dataset=dataset_for_lm['test'], # we use the test set as validation set\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator is used to create batches from data. \n",
    "    # When a tokenizer is passed the default to DataCollatorWithPadding is used.\n",
    "    # So we change it since our model do not use PAD tokens\n",
    "    data_collator=default_data_collator,\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training on GPU\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "# Use tensorboard to monitor the training\n",
    "# Load the TensorBoard notebook extension\n",
    "%reload_ext tensorboard  \n",
    "\n",
    " # read data from tensorboard dir\n",
    "%tensorboard --logdir gpt2-jokes/tb "
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "# Finally: let's start the training!\n",
    "train_results = trainer.train()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2802\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 350\n",
      " 20%|██        | 71/350 [01:32<06:09,  1.32s/it]"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('NLP': conda)"
  },
  "interpreter": {
   "hash": "86c899ea07089cc9d284ea9f9b0758fa4e79f644b216f9a03b961d7e004425f1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}